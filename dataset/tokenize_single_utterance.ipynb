{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritsuko/ai/audio/dual-ar/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, DatasetDict, concatenate_datasets\n",
    "\n",
    "# dataset = load_from_disk(\"encoded_dataset\")\n",
    "train_clean_100 = load_from_disk(\"encoded_libritts/train.clean.100/\")\n",
    "train_clean_360 = load_from_disk(\"encoded_libritts/train.clean.360/\")\n",
    "dev_clean = load_from_disk(\"encoded_libritts/dev.clean\")\n",
    "test_clean = load_from_disk(\"encoded_libritts/test.clean\")\n",
    "full_train = concatenate_datasets([train_clean_100, train_clean_360])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": full_train,\n",
    "    \"val\": dev_clean,\n",
    "    \"test\": test_clean\n",
    "})\n",
    "dataset = dataset.with_format(\"torch\")\n",
    "dataset = dataset.remove_columns([\"path\", \"chapter_id\", \"text_original\"])\n",
    "dataset = dataset.rename_column(original_column_name=\"text_normalized\", new_column_name=\"normalized_text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "def make_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
    "    semantic_tokens = [f\"<|semantic:{i}|>\" for i in range(0,2048)]\n",
    "    additional_special_tokens = [*semantic_tokens]\n",
    "    tokenizer.add_special_tokens({\n",
    "        \"additional_special_tokens\": additional_special_tokens\n",
    "    })\n",
    "    tokenizer.save_pretrained(\"../checkpoints/smoltts\")\n",
    "\n",
    "# make_tokenizer()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../checkpoints/smoltts\")\n",
    "tokenizer.use_default_system_prompt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51200, 49152)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer), tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'normalized_text': 'I felt it in my bones when I woke this morning that something splendid was going to turn up.',\n",
       " 'speaker_id': '4446',\n",
       " 'id': '4446_2275_000002_000009',\n",
       " 'codes': tensor([[1049, 1268,  549, 1324,  668, 1538, 1593,   95,  629, 1281, 1281,  680,\n",
       "           536,  536,  230, 1018, 1117,  244,  507,  997, 1399,  640, 1591, 1967,\n",
       "          1161,  690,   67, 1772,  830, 1612,  561,  119, 1052,  880, 1029, 1532,\n",
       "          1161, 1344, 1109,    6, 1001,  382,  596,   99, 1726, 2030,  531,  616,\n",
       "           367, 1271, 1868,  978,  729,  396, 1544],\n",
       "         [1470, 1879,  712,  283,  220,  137, 1610,  263,  531, 1845, 1428, 1132,\n",
       "           359, 1904, 1458, 1876,  895,  149,  190,  116,  603,  786, 1884, 1455,\n",
       "          1928,  677,  914, 1122,  436,  618,  850, 1766, 2005, 1618,  966,  850,\n",
       "          1663,  172,  274,  612, 1013, 1928, 1262, 1169, 1006, 1777, 1755, 2026,\n",
       "          1714,  788,  786, 1520,  811,   91, 1700],\n",
       "         [ 373,  602, 2016, 1148,   98,  790, 1570,  944, 1872,  807, 1427,  817,\n",
       "           602, 1090,  341, 1609,  774,  327,  479,  229, 1988, 1596, 1595,  339,\n",
       "          1801, 1360, 1415,  492,    3, 1064,  378, 1530,  800, 1369, 1331,  782,\n",
       "          1822,  511,  349, 1534, 1245, 1788,  146, 1195, 1510,  584, 1782,  662,\n",
       "          1523, 1523,   68, 2016, 1240,  783, 1178],\n",
       "         [1225, 1475,   17,  148,  656, 2036,  663,  632, 1926,  358, 1443,   33,\n",
       "          1316, 1993, 1038,  324, 1240,  861,  787, 1994, 1721,  832,  707,  435,\n",
       "           113,  138,  494,  920, 1707, 1134, 1003,  842, 1511,  193, 1936, 1478,\n",
       "          1261,  332,  756,  115, 1700,  324, 1074, 1631, 1575,  526, 1957,  756,\n",
       "           318,   19,  860, 1562,  164, 1477, 1450],\n",
       "         [ 457, 2035, 1113,  504, 1301,    5,  739,  334,  178, 1832, 1803,  266,\n",
       "          1719,  516,  147, 1016, 1402,  709, 1046, 1620,  440,  996,  100,  551,\n",
       "          1799, 1930, 1811, 1551, 1026, 1440,  684,  313,   13,  629, 1930,  723,\n",
       "           457,  328, 1455,  796,  300, 1610,  435,  635, 1517,  958, 1607, 1013,\n",
       "           911,  958,  524,  457, 1019, 1019,  481],\n",
       "         [1547,  592,  601, 2022, 1795,   42, 1631, 1174,  496, 1640,  935, 1687,\n",
       "           654, 1105,  668, 1995,  918, 1002, 1668,  190, 1148, 1509, 1563,   30,\n",
       "          1669,  882, 1529, 1794, 1086,  900,  274, 1001,  482,   42, 1852,  780,\n",
       "           704,  275, 1073, 1174, 1618, 1540, 1344, 1371,  263, 1245, 1382,  253,\n",
       "           484, 2044, 1717, 1942,  165, 1832,  478],\n",
       "         [1433, 1154, 1672,  879, 1683, 1529,  561,  508, 1648,  358,  646, 1289,\n",
       "          1307, 1867, 1937, 1076, 1694, 1730, 1199, 1758, 1476,  369,  801, 1249,\n",
       "           260, 1759,  740,   53,  618, 2023,  119,  835, 2015,  835,   66, 1209,\n",
       "          1285, 1744,  804, 1863, 1240,  392,  741, 1834, 1193, 1440, 1696, 1470,\n",
       "          1810, 1464,  840,   43, 1978,  666, 1978],\n",
       "         [ 948,  232, 1109, 1286, 1371,   94, 1516,  700,  714,  537, 1939,  334,\n",
       "            17, 1093, 1769, 1511, 1063,  444, 1399, 1594,   10,  532,  208, 1803,\n",
       "           945, 1193, 1082, 1667,  198,  382,  101, 1426,  458, 1587, 1450, 1722,\n",
       "          1173, 1286,  127,  225, 1482, 1368,  370, 1003, 1808, 1772,  952,  992,\n",
       "          1977,  820,  481,  945,  945, 1477, 1665]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the tokenizer by encoding and decoding some example text\n",
    "example_text = \"This is a test sentence.\"\n",
    "encoded = tokenizer(example_text, return_tensors=\"pt\")\n",
    "decoded = tokenizer.decode(encoded['input_ids'][0])\n",
    "\n",
    "# Print the results\n",
    "dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def encode_text(role: str, content: str, needs_initial_newline=False) -> torch.Tensor:\n",
    "    sys_line = tokenizer.encode(f\"{chr(10) if needs_initial_newline else ''}<|im_start|>{role}\\n{content}<|im_end|>\\n\", return_tensors=\"pt\")\n",
    "    zeros_mask = torch.zeros(8, sys_line.size(1), dtype=sys_line.dtype)\n",
    "    return torch.cat([sys_line, zeros_mask])\n",
    "\n",
    "tts_sysprompt = encode_text(\"system\", \"Speak out the provided text\")\n",
    "asr_sysprompt = encode_text(\"system\", \"Transcribe the provided speech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>assistant\\n<|semantic:1049|><|semantic:1268|><|semantic:549|><|semantic:1324|><|semantic:668|><|semantic:1538|><|semantic:1593|><|semantic:95|><|semantic:629|><|semantic:1281|><|semantic:1281|><|semantic:680|><|semantic:536|><|semantic:536|><|semantic:230|><|semantic:1018|><|semantic:1117|><|semantic:244|><|semantic:507|><|semantic:997|><|semantic:1399|><|semantic:640|><|semantic:1591|><|semantic:1967|><|semantic:1161|><|semantic:690|><|semantic:67|><|semantic:1772|><|semantic:830|><|semantic:1612|><|semantic:561|><|semantic:119|><|semantic:1052|><|semantic:880|><|semantic:1029|><|semantic:1532|><|semantic:1161|><|semantic:1344|><|semantic:1109|><|semantic:6|><|semantic:1001|><|semantic:382|><|semantic:596|><|semantic:99|><|semantic:1726|><|semantic:2030|><|semantic:531|><|semantic:616|><|semantic:367|><|semantic:1271|><|semantic:1868|><|semantic:978|><|semantic:729|><|semantic:396|><|semantic:1544|><|im_end|>'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEMANTIC_OFFSET = tokenizer.encode(\"<|semantic:0|>\")[0]\n",
    "VQ_ASSISTANT_WRAPPER = encode_text(role=\"assistant\", content=\"\")[:,-7:-1]\n",
    "VQ_USER_WRAPPER = encode_text(role=\"user\", content=\"\")[:,:-1]\n",
    "\n",
    "def encode_vq(codes: torch.Tensor, is_assistant=True) -> torch.Tensor:\n",
    "    speaker_line = codes[0,:] + SEMANTIC_OFFSET\n",
    "    vq_block = torch.cat([speaker_line.unsqueeze(0), codes])\n",
    "    wrapper = VQ_ASSISTANT_WRAPPER if is_assistant else VQ_USER_WRAPPER\n",
    "    # print(f\"VQ BLOCK: {vq_block.shape}, WRAPPER: {wrapper.shape}, is_assistant: {is_assistant}\")\n",
    "    return torch.cat([wrapper[:,:-1], vq_block, wrapper[:,-1].unsqueeze(1)], dim=1)\n",
    "\n",
    "\n",
    "out = encode_vq(dataset[\"test\"][0][\"codes\"], is_assistant=True)\n",
    "tokenizer.decode(out[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nSpeak out the provided text<|im_end|>\\n<|im_start|>user\\nI felt it in my bones when I woke this morning that something splendid was going to turn up.<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1049|><|semantic:1268|><|semantic:549|><|semantic:1324|><|semantic:668|><|semantic:1538|><|semantic:1593|><|semantic:95|><|semantic:629|><|semantic:1281|><|semantic:1281|><|semantic:680|><|semantic:536|><|semantic:536|><|semantic:230|><|semantic:1018|><|semantic:1117|><|semantic:244|><|semantic:507|><|semantic:997|><|semantic:1399|><|semantic:640|><|semantic:1591|><|semantic:1967|><|semantic:1161|><|semantic:690|><|semantic:67|><|semantic:1772|><|semantic:830|><|semantic:1612|><|semantic:561|><|semantic:119|><|semantic:1052|><|semantic:880|><|semantic:1029|><|semantic:1532|><|semantic:1161|><|semantic:1344|><|semantic:1109|><|semantic:6|><|semantic:1001|><|semantic:382|><|semantic:596|><|semantic:99|><|semantic:1726|><|semantic:2030|><|semantic:531|><|semantic:616|><|semantic:367|><|semantic:1271|><|semantic:1868|><|semantic:978|><|semantic:729|><|semantic:396|><|semantic:1544|>'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "ASSISTANT_PREFIX_LEN = len(tokenizer.tokenize(\"<|im_start|>assistant\\n\"))\n",
    "USER_PREFIX_LEN  = len(tokenizer.tokenize(\"<|im_start|>user\\n\"))\n",
    "\n",
    "def tokenize_row(row: Dict, is_batch=True):\n",
    "    \"\"\"\n",
    "    row[\"normalized_text\"] is a string\n",
    "    row[\"codes\"] is a torch.Tensor shaped [9, T_vq]\n",
    "    \"\"\"\n",
    "    row = {\n",
    "        \"normalized_text\": row[\"normalized_text\"][0],\n",
    "        \"codes\": row[\"codes\"][0],\n",
    "        \"speaker_id\": row[\"speaker_id\"],\n",
    "        \"id\": row[\"id\"]\n",
    "    } if is_batch else row\n",
    "    tts_user_line = encode_text(role=\"user\", content=row[\"normalized_text\"])\n",
    "    asr_assistant_line = encode_text(role=\"assistant\", content=row[\"normalized_text\"], needs_initial_newline=True)\n",
    "    tts_assistant_codes = encode_vq(row[\"codes\"])  # shape [9, T_vq]\n",
    "    asr_user_codes = encode_vq(row[\"codes\"], is_assistant=False)  # shape [9, T_vq]\n",
    "    \n",
    "    # Concatenate system prompt (row=1?), user line (row=1?), codebooks (row=9),\n",
    "    # but along the *time* dimension => final shape [9, T_total] \n",
    "    #   (since sysprompt and user_line are [1, T_something], \n",
    "    #    codes_9rows is [9, T_vq], so we pad them to 9 rows if needed)\n",
    "    # For demonstration, I'm just stacking them. You probably do:\n",
    "    tts_ground_truth = torch.cat([tts_sysprompt, tts_user_line, tts_assistant_codes], dim=1)\n",
    "    asr_ground_truth = torch.cat([asr_sysprompt, asr_user_codes, asr_assistant_line], dim=1)\n",
    "    tts_tokens = tts_ground_truth[:,:-1].clone()\n",
    "    asr_tokens = asr_ground_truth[:,:-1].clone()\n",
    "    # Clone for labels\n",
    "    tts_labels = tts_ground_truth[:, 1:].clone()\n",
    "    asr_labels = asr_ground_truth[:, 1:].clone()\n",
    "\n",
    "    # TTS MASKING (easy)\n",
    "    # labels = asr_ground_truth[:, 1:].clone()\n",
    "    # Let's define the \"text portion\" as sysprompt + user_line only\n",
    "    text_len = tts_sysprompt.size(1) + tts_user_line.size(1) + ASSISTANT_PREFIX_LEN - 1  # no VQ_WRAPPER or codes\n",
    "    # ONLY mask codebook rows for that text region\n",
    "    # row=0 is your \"text\" row, row=1..8 might be codebooks, or vice versa\n",
    "    # (Here I'm assuming row=0 is your actual text tokens. \n",
    "    #  If it's reversed, tweak accordingly!)\n",
    "    tts_labels[1:, :text_len] = -100\n",
    "\n",
    "    asr_start_len = asr_sysprompt.size(1) + USER_PREFIX_LEN - 1\n",
    "    asr_labels[1:, :asr_start_len] = -100\n",
    "    asr_labels[1:, -asr_assistant_line.size(1):] = -100\n",
    "\n",
    "    out = {\n",
    "        \"tokens\": [tts_tokens, asr_tokens],\n",
    "        \"labels\": [tts_labels, asr_labels],\n",
    "        \"task\": [\"tts\", \"asr\"],\n",
    "        \"normalized_text\": [row[\"normalized_text\"]] * 2,\n",
    "        \"speaker_id\": row[\"speaker_id\"] * 2,\n",
    "        \"id\": row[\"id\"] * 2,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "example_row = tokenize_row(dataset[\"test\"][0], is_batch=False)\n",
    "tokenizer.decode(example_row[\"tokens\"][0][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'normalized_text': '[The moon] I gazed with a kind of wonder.',\n",
       " 'speaker_id': '730',\n",
       " 'id': '730_358_000003_000002',\n",
       " 'codes': tensor([[1049, 1102, 1686, 1258, 1258, 1689, 1528, 1987,  978,  312, 2039,  753,\n",
       "           969,  598, 1084, 1268,  621, 1757,  560, 1734, 1527, 1117,  622,  628,\n",
       "           510,  623,  623,  918,  689,  997, 1069, 1941,  294,  774,  518, 1987,\n",
       "           769],\n",
       "         [ 363, 1427, 1427, 1427, 1525, 1452, 1332,  363,   91,  441,  441,  441,\n",
       "           363,  363,  212, 1834, 2005,  144, 1507, 1772, 1501,  786, 1891,  543,\n",
       "          1646, 1181, 1097,  941, 1028,    8, 1097, 1772, 1252, 1731, 1182, 1520,\n",
       "           243],\n",
       "         [ 373, 1031,  314, 1916,   17,  681, 1370, 2016, 2016, 1400,  457, 2016,\n",
       "          1569, 1569, 1796, 1350, 1370,   88,   88,   88,  311,  329, 1738, 1822,\n",
       "          1017, 2025, 1453,  324,  568, 1787, 1517,  999,  461,   61,  231, 2016,\n",
       "           265],\n",
       "         [1477,  176, 1056, 2024, 1269,  349,  104, 1521,  417, 1422,  125, 1229,\n",
       "           525, 1868, 1580,  690, 1381,  157, 1168, 1069, 1580,  196, 2012, 1072,\n",
       "           813, 1567,  810, 1201, 1441, 1738,  967, 1305, 1227, 1029, 1521,  451,\n",
       "           164],\n",
       "         [ 894, 1927, 1896, 1185, 1409,  447, 1684,   74, 1335, 1019, 1517,   68,\n",
       "            68, 1532, 1065, 1855,  441, 1260, 1235,  528, 1065, 1385, 1634, 1426,\n",
       "           143,   49,  397,  487, 1801,  300,   26, 1507,  993,  619, 1927, 1928,\n",
       "           457],\n",
       "         [ 478, 1052,  590, 1476,   91,  190, 1421, 1867, 1350, 1509, 1832,  165,\n",
       "           317, 1509,   93,  271, 1365,  271, 1805, 1504, 1767, 1645,  413, 1440,\n",
       "          1937, 1393,  992,  293,  154, 1563,  520, 1955, 1547,  488, 1293,  602,\n",
       "          1509],\n",
       "         [1234, 1824, 1164, 1745,  150, 1750,  568, 1417, 1044,  168,   43,  469,\n",
       "          1249, 1978, 1175,  325, 1807, 1536, 1721, 1625, 1652, 1726, 1523,  112,\n",
       "           326,  337, 2023, 1770, 1042,  721,  714, 1744, 1158,  840, 1425, 1044,\n",
       "          1871],\n",
       "         [1137, 1786,  788,  851, 1841, 1267,  275,  944, 1868, 1477, 1808, 1808,\n",
       "           945, 1177, 1098,  419, 1567,   21, 1982,  315,  697,  908,  445, 1761,\n",
       "          1269, 1625,  986, 1842,  515, 1204,  986,  258, 1659, 1270,  697,  152,\n",
       "           583]])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/149658 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 149658/149658 [03:06<00:00, 803.39 examples/s]\n",
      "Map: 100%|██████████| 5736/5736 [00:07<00:00, 756.32 examples/s]\n",
      "Map: 100%|██████████| 4837/4837 [00:06<00:00, 734.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT INCREASE batch size\n",
    "dataset = dataset.map(tokenize_row, remove_columns=\"codes\", batched=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (6/6 shards): 100%|██████████| 149658/149658 [00:01<00:00, 97881.60 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5736/5736 [00:00<00:00, 101659.04 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4837/4837 [00:00<00:00, 89127.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"tokenized_libritts_bijection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nTranscribe the provided speech<|im_end|>\\n<|im_start|>user\\n<|semantic:1049|><|semantic:1268|><|semantic:549|><|semantic:1324|><|semantic:668|><|semantic:1538|><|semantic:1593|><|semantic:95|><|semantic:629|><|semantic:1281|><|semantic:1281|><|semantic:680|><|semantic:536|><|semantic:536|><|semantic:230|><|semantic:1018|><|semantic:1117|><|semantic:244|><|semantic:507|><|semantic:997|><|semantic:1399|><|semantic:640|><|semantic:1591|><|semantic:1967|><|semantic:1161|><|semantic:690|><|semantic:67|><|semantic:1772|><|semantic:830|><|semantic:1612|><|semantic:561|><|semantic:119|><|semantic:1052|><|semantic:880|><|semantic:1029|><|semantic:1532|><|semantic:1161|><|semantic:1344|><|semantic:1109|><|semantic:6|><|semantic:1001|><|semantic:382|><|semantic:596|><|semantic:99|><|semantic:1726|><|semantic:2030|><|semantic:531|><|semantic:616|><|semantic:367|><|semantic:1271|><|semantic:1868|><|semantic:978|><|semantic:729|><|semantic:396|><|semantic:1544|><|im_end|>\\n<|im_start|>assistant\\nI felt it in my bones when I woke this morning that something splendid was going to turn up.<|im_end|>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example_row[1][\"tokens\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system\\nSpeak out the provided text<|im_end|>\\n<|im_start|>user\\nI felt it in my bones when I woke this morning that something splendid was going to turn up.<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1049|><|semantic:1268|><|semantic:549|><|semantic:1324|><|semantic:668|><|semantic:1538|><|semantic:1593|><|semantic:95|><|semantic:629|><|semantic:1281|><|semantic:1281|><|semantic:680|><|semantic:536|><|semantic:536|><|semantic:230|><|semantic:1018|><|semantic:1117|><|semantic:244|><|semantic:507|><|semantic:997|><|semantic:1399|><|semantic:640|><|semantic:1591|><|semantic:1967|><|semantic:1161|><|semantic:690|><|semantic:67|><|semantic:1772|><|semantic:830|><|semantic:1612|><|semantic:561|><|semantic:119|><|semantic:1052|><|semantic:880|><|semantic:1029|><|semantic:1532|><|semantic:1161|><|semantic:1344|><|semantic:1109|><|semantic:6|><|semantic:1001|><|semantic:382|><|semantic:596|><|semantic:99|><|semantic:1726|><|semantic:2030|><|semantic:531|><|semantic:616|><|semantic:367|><|semantic:1271|><|semantic:1868|><|semantic:978|><|semantic:729|><|semantic:396|><|semantic:1544|><|im_end|>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example_row[0][\"labels\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system\\nTranscribe the provided speech<|im_end|>\\n<|im_start|>user\\n<|semantic:1049|><|semantic:1268|><|semantic:549|><|semantic:1324|><|semantic:668|><|semantic:1538|><|semantic:1593|><|semantic:95|><|semantic:629|><|semantic:1281|><|semantic:1281|><|semantic:680|><|semantic:536|><|semantic:536|><|semantic:230|><|semantic:1018|><|semantic:1117|><|semantic:244|><|semantic:507|><|semantic:997|><|semantic:1399|><|semantic:640|><|semantic:1591|><|semantic:1967|><|semantic:1161|><|semantic:690|><|semantic:67|><|semantic:1772|><|semantic:830|><|semantic:1612|><|semantic:561|><|semantic:119|><|semantic:1052|><|semantic:880|><|semantic:1029|><|semantic:1532|><|semantic:1161|><|semantic:1344|><|semantic:1109|><|semantic:6|><|semantic:1001|><|semantic:382|><|semantic:596|><|semantic:99|><|semantic:1726|><|semantic:2030|><|semantic:531|><|semantic:616|><|semantic:367|><|semantic:1271|><|semantic:1868|><|semantic:978|><|semantic:729|><|semantic:396|><|semantic:1544|><|im_end|>\\n<|im_start|>assistant\\nI felt it in my bones when I woke this morning that something splendid was going to turn up.<|im_end|>\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example_row[1][\"labels\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0, 1698, 1719,  204, 1389,  851, 1772,  186, 1307, 1895,  832,\n",
       "        1633,  771,  648, 1530, 1989, 1574, 1348,  722,  144, 1945,  278, 1109,\n",
       "          29,  611,   46,  622,  628, 1740,  572,  572,  345, 1989, 1676,  929,\n",
       "        1776,  749,  313, 1997, 1571,  819, 1238, 1054, 1054, 1135, 1506, 1393,\n",
       "         616, 1702,  993,  579,  486,  486, 2039,  148,  657,  664,  339,  339,\n",
       "         588,  212, 1443,   32, 1320, 1549,  440,    8, 1407, 1722, 1650, 1615,\n",
       "         798,  121,  303,  697,  837,  358, 1882,  440, 1992, 1992,  587,  178,\n",
       "         178, 1627, 1530,  929, 1610, 1916,  523,  213, 1252, 1480, 1468, 1899,\n",
       "         773, 2033, 2033,   83, 1146,  784, 1295,  199, 1109,  268,    6,    6,\n",
       "        1781, 1479, 1530, 1530,  146, 2038,  984, 1403,  606, 1379, 1840, 1172,\n",
       "        1680, 1162, 1928])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_row[\"tokens\"][0][1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, 1049, 1268,  549, 1324,  668, 1538, 1593,   95,  629, 1281, 1281,\n",
       "         680,  536,  536,  230, 1018, 1117,  244,  507,  997, 1399,  640, 1591,\n",
       "        1967, 1161,  690,   67, 1772,  830, 1612,  561,  119, 1052,  880, 1029,\n",
       "        1532, 1161, 1344, 1109,    6, 1001,  382,  596,   99, 1726, 2030,  531,\n",
       "         616,  367, 1271, 1868,  978,  729,  396, 1544,    0, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_row[1][\"labels\"][1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'normalized_text': 'I felt it in my bones when I woke this morning that something splendid was going to turn up.',\n",
       " 'speaker_id': '4446',\n",
       " 'id': '4446_2275_000002_000009',\n",
       " 'tokens': tensor([[    1,  9690,   198, 15024,   494,   578,   260,  2711,  1694,     2,\n",
       "            198,     1,  4093,   198,    57,  4592,   357,   281,   957,  6542,\n",
       "            645,   339, 40652,   451,  5738,   338,  1488, 33494,   436,  2045,\n",
       "            288,  1607,   614,    30,     2,   198,     1,   520,  9531,   198,\n",
       "          50201, 50420, 49701, 50476, 49820, 50690, 50745, 49247, 49781, 50433,\n",
       "          50433, 49832, 49688, 49688, 49382, 50170, 50269, 49396, 49659, 50149,\n",
       "          50551, 49792, 50743, 51119, 50313, 49842, 49219, 50924, 49982, 50764,\n",
       "          49713, 49271, 50204, 50032, 50181, 50684, 50313, 50496, 50261, 49158,\n",
       "          50153, 49534, 49748, 49251, 50878, 51182, 49683, 49768, 49519, 50423,\n",
       "          51020, 50130, 49881, 49548, 50696],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1049,  1268,   549,  1324,   668,  1538,  1593,    95,   629,  1281,\n",
       "           1281,   680,   536,   536,   230,  1018,  1117,   244,   507,   997,\n",
       "           1399,   640,  1591,  1967,  1161,   690,    67,  1772,   830,  1612,\n",
       "            561,   119,  1052,   880,  1029,  1532,  1161,  1344,  1109,     6,\n",
       "           1001,   382,   596,    99,  1726,  2030,   531,   616,   367,  1271,\n",
       "           1868,   978,   729,   396,  1544],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1470,  1879,   712,   283,   220,   137,  1610,   263,   531,  1845,\n",
       "           1428,  1132,   359,  1904,  1458,  1876,   895,   149,   190,   116,\n",
       "            603,   786,  1884,  1455,  1928,   677,   914,  1122,   436,   618,\n",
       "            850,  1766,  2005,  1618,   966,   850,  1663,   172,   274,   612,\n",
       "           1013,  1928,  1262,  1169,  1006,  1777,  1755,  2026,  1714,   788,\n",
       "            786,  1520,   811,    91,  1700],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            373,   602,  2016,  1148,    98,   790,  1570,   944,  1872,   807,\n",
       "           1427,   817,   602,  1090,   341,  1609,   774,   327,   479,   229,\n",
       "           1988,  1596,  1595,   339,  1801,  1360,  1415,   492,     3,  1064,\n",
       "            378,  1530,   800,  1369,  1331,   782,  1822,   511,   349,  1534,\n",
       "           1245,  1788,   146,  1195,  1510,   584,  1782,   662,  1523,  1523,\n",
       "             68,  2016,  1240,   783,  1178],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1225,  1475,    17,   148,   656,  2036,   663,   632,  1926,   358,\n",
       "           1443,    33,  1316,  1993,  1038,   324,  1240,   861,   787,  1994,\n",
       "           1721,   832,   707,   435,   113,   138,   494,   920,  1707,  1134,\n",
       "           1003,   842,  1511,   193,  1936,  1478,  1261,   332,   756,   115,\n",
       "           1700,   324,  1074,  1631,  1575,   526,  1957,   756,   318,    19,\n",
       "            860,  1562,   164,  1477,  1450],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            457,  2035,  1113,   504,  1301,     5,   739,   334,   178,  1832,\n",
       "           1803,   266,  1719,   516,   147,  1016,  1402,   709,  1046,  1620,\n",
       "            440,   996,   100,   551,  1799,  1930,  1811,  1551,  1026,  1440,\n",
       "            684,   313,    13,   629,  1930,   723,   457,   328,  1455,   796,\n",
       "            300,  1610,   435,   635,  1517,   958,  1607,  1013,   911,   958,\n",
       "            524,   457,  1019,  1019,   481],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1547,   592,   601,  2022,  1795,    42,  1631,  1174,   496,  1640,\n",
       "            935,  1687,   654,  1105,   668,  1995,   918,  1002,  1668,   190,\n",
       "           1148,  1509,  1563,    30,  1669,   882,  1529,  1794,  1086,   900,\n",
       "            274,  1001,   482,    42,  1852,   780,   704,   275,  1073,  1174,\n",
       "           1618,  1540,  1344,  1371,   263,  1245,  1382,   253,   484,  2044,\n",
       "           1717,  1942,   165,  1832,   478],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1433,  1154,  1672,   879,  1683,  1529,   561,   508,  1648,   358,\n",
       "            646,  1289,  1307,  1867,  1937,  1076,  1694,  1730,  1199,  1758,\n",
       "           1476,   369,   801,  1249,   260,  1759,   740,    53,   618,  2023,\n",
       "            119,   835,  2015,   835,    66,  1209,  1285,  1744,   804,  1863,\n",
       "           1240,   392,   741,  1834,  1193,  1440,  1696,  1470,  1810,  1464,\n",
       "            840,    43,  1978,   666,  1978],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            948,   232,  1109,  1286,  1371,    94,  1516,   700,   714,   537,\n",
       "           1939,   334,    17,  1093,  1769,  1511,  1063,   444,  1399,  1594,\n",
       "             10,   532,   208,  1803,   945,  1193,  1082,  1667,   198,   382,\n",
       "            101,  1426,   458,  1587,  1450,  1722,  1173,  1286,   127,   225,\n",
       "           1482,  1368,   370,  1003,  1808,  1772,   952,   992,  1977,   820,\n",
       "            481,   945,   945,  1477,  1665]]),\n",
       " 'labels': tensor([[ 9690,   198, 15024,   494,   578,   260,  2711,  1694,     2,   198,\n",
       "              1,  4093,   198,    57,  4592,   357,   281,   957,  6542,   645,\n",
       "            339, 40652,   451,  5738,   338,  1488, 33494,   436,  2045,   288,\n",
       "           1607,   614,    30,     2,   198,     1,   520,  9531,   198, 50201,\n",
       "          50420, 49701, 50476, 49820, 50690, 50745, 49247, 49781, 50433, 50433,\n",
       "          49832, 49688, 49688, 49382, 50170, 50269, 49396, 49659, 50149, 50551,\n",
       "          49792, 50743, 51119, 50313, 49842, 49219, 50924, 49982, 50764, 49713,\n",
       "          49271, 50204, 50032, 50181, 50684, 50313, 50496, 50261, 49158, 50153,\n",
       "          49534, 49748, 49251, 50878, 51182, 49683, 49768, 49519, 50423, 51020,\n",
       "          50130, 49881, 49548, 50696,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1049,\n",
       "           1268,   549,  1324,   668,  1538,  1593,    95,   629,  1281,  1281,\n",
       "            680,   536,   536,   230,  1018,  1117,   244,   507,   997,  1399,\n",
       "            640,  1591,  1967,  1161,   690,    67,  1772,   830,  1612,   561,\n",
       "            119,  1052,   880,  1029,  1532,  1161,  1344,  1109,     6,  1001,\n",
       "            382,   596,    99,  1726,  2030,   531,   616,   367,  1271,  1868,\n",
       "            978,   729,   396,  1544,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1470,\n",
       "           1879,   712,   283,   220,   137,  1610,   263,   531,  1845,  1428,\n",
       "           1132,   359,  1904,  1458,  1876,   895,   149,   190,   116,   603,\n",
       "            786,  1884,  1455,  1928,   677,   914,  1122,   436,   618,   850,\n",
       "           1766,  2005,  1618,   966,   850,  1663,   172,   274,   612,  1013,\n",
       "           1928,  1262,  1169,  1006,  1777,  1755,  2026,  1714,   788,   786,\n",
       "           1520,   811,    91,  1700,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   373,\n",
       "            602,  2016,  1148,    98,   790,  1570,   944,  1872,   807,  1427,\n",
       "            817,   602,  1090,   341,  1609,   774,   327,   479,   229,  1988,\n",
       "           1596,  1595,   339,  1801,  1360,  1415,   492,     3,  1064,   378,\n",
       "           1530,   800,  1369,  1331,   782,  1822,   511,   349,  1534,  1245,\n",
       "           1788,   146,  1195,  1510,   584,  1782,   662,  1523,  1523,    68,\n",
       "           2016,  1240,   783,  1178,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1225,\n",
       "           1475,    17,   148,   656,  2036,   663,   632,  1926,   358,  1443,\n",
       "             33,  1316,  1993,  1038,   324,  1240,   861,   787,  1994,  1721,\n",
       "            832,   707,   435,   113,   138,   494,   920,  1707,  1134,  1003,\n",
       "            842,  1511,   193,  1936,  1478,  1261,   332,   756,   115,  1700,\n",
       "            324,  1074,  1631,  1575,   526,  1957,   756,   318,    19,   860,\n",
       "           1562,   164,  1477,  1450,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   457,\n",
       "           2035,  1113,   504,  1301,     5,   739,   334,   178,  1832,  1803,\n",
       "            266,  1719,   516,   147,  1016,  1402,   709,  1046,  1620,   440,\n",
       "            996,   100,   551,  1799,  1930,  1811,  1551,  1026,  1440,   684,\n",
       "            313,    13,   629,  1930,   723,   457,   328,  1455,   796,   300,\n",
       "           1610,   435,   635,  1517,   958,  1607,  1013,   911,   958,   524,\n",
       "            457,  1019,  1019,   481,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1547,\n",
       "            592,   601,  2022,  1795,    42,  1631,  1174,   496,  1640,   935,\n",
       "           1687,   654,  1105,   668,  1995,   918,  1002,  1668,   190,  1148,\n",
       "           1509,  1563,    30,  1669,   882,  1529,  1794,  1086,   900,   274,\n",
       "           1001,   482,    42,  1852,   780,   704,   275,  1073,  1174,  1618,\n",
       "           1540,  1344,  1371,   263,  1245,  1382,   253,   484,  2044,  1717,\n",
       "           1942,   165,  1832,   478,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1433,\n",
       "           1154,  1672,   879,  1683,  1529,   561,   508,  1648,   358,   646,\n",
       "           1289,  1307,  1867,  1937,  1076,  1694,  1730,  1199,  1758,  1476,\n",
       "            369,   801,  1249,   260,  1759,   740,    53,   618,  2023,   119,\n",
       "            835,  2015,   835,    66,  1209,  1285,  1744,   804,  1863,  1240,\n",
       "            392,   741,  1834,  1193,  1440,  1696,  1470,  1810,  1464,   840,\n",
       "             43,  1978,   666,  1978,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   948,\n",
       "            232,  1109,  1286,  1371,    94,  1516,   700,   714,   537,  1939,\n",
       "            334,    17,  1093,  1769,  1511,  1063,   444,  1399,  1594,    10,\n",
       "            532,   208,  1803,   945,  1193,  1082,  1667,   198,   382,   101,\n",
       "           1426,   458,  1587,  1450,  1722,  1173,  1286,   127,   225,  1482,\n",
       "           1368,   370,  1003,  1808,  1772,   952,   992,  1977,   820,   481,\n",
       "            945,   945,  1477,  1665,     0]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
