{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritsuko/ai/audio/dual-ar/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"encoded_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "def make_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
    "    semantic_tokens = [f\"<|semantic:{i}|>\" for i in range(0,2048)]\n",
    "    additional_special_tokens = [*semantic_tokens]\n",
    "    tokenizer.add_special_tokens({\n",
    "        \"additional_special_tokens\": additional_special_tokens\n",
    "    })\n",
    "    tokenizer.save_pretrained(\"../checkpoints/smoltts\")\n",
    "\n",
    "# make_tokenizer()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../checkpoints/smoltts\")\n",
    "tokenizer.use_default_system_prompt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51200, 49152)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer), tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'lj-001-0001.wav',\n",
       " 'spoken_text': 'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition',\n",
       " 'normalized_text': 'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition',\n",
       " 'codes': tensor([[1698, 1719,  204, 1389,  851, 1772,  186, 1307, 1895,  832, 1633,  771,\n",
       "           648, 1530, 1989, 1574, 1348,  722,  144, 1945,  278, 1109,   29,  611,\n",
       "            46,  622,  628, 1740,  572,  572,  345, 1989, 1676,  929, 1776,  749,\n",
       "           313, 1997, 1571,  819, 1238, 1054, 1054, 1135, 1506, 1393,  616, 1702,\n",
       "           993,  579,  486,  486, 2039,  148,  657,  664,  339,  339,  588,  212,\n",
       "          1443,   32, 1320, 1549,  440,    8, 1407, 1722, 1650, 1615,  798,  121,\n",
       "           303,  697,  837,  358, 1882,  440, 1992, 1992,  587,  178,  178, 1627,\n",
       "          1530,  929, 1610, 1916,  523,  213, 1252, 1480, 1468, 1899,  773, 2033,\n",
       "          2033,   83, 1146,  784, 1295,  199, 1109,  268,    6,    6, 1781, 1479,\n",
       "          1530, 1530,  146, 2038,  984, 1403,  606, 1379, 1840, 1172, 1680, 1162,\n",
       "          1928],\n",
       "         [1293, 1310, 1535, 1766, 1004, 1313,  448, 1253,  907, 1050, 1153, 1122,\n",
       "          1077, 1279,  523, 1624, 1839,  122, 1537, 1101,  666,  159, 1523, 1101,\n",
       "           196,  298, 1234, 1958, 1668, 1094,  551, 1157, 1461, 2047,  410,  254,\n",
       "           783, 1410,   28,  714,  921,  421, 1462, 1995, 1125, 1847, 2032, 1011,\n",
       "          1523, 1546, 1211, 1050, 1056, 1056, 1056, 1611,  250, 1264, 1742,   57,\n",
       "          2024, 1496, 1801, 1638, 1985, 1839, 1681, 1138,  561, 2024,  302, 1027,\n",
       "          1776, 1539, 1138,  549, 1197, 1019, 1348,  254,  931,  892, 1019, 1836,\n",
       "          1004,  351, 1246, 1862, 1800,  884, 1349,  138,  643,  982, 1188,  497,\n",
       "          1143,  724, 1681,  815, 1228, 1876, 1911, 1668, 1615,  786, 1519,  690,\n",
       "          1750, 1212, 1270,  352,    9, 1276, 2035, 1231, 1530, 1232,  448,  907,\n",
       "          1159],\n",
       "         [ 850,  901, 1258,  692,  532,  632,  988,  462,  462, 1697,  259, 1992,\n",
       "           875,  229, 1769, 1087, 1725, 1353, 1738,  692,  407, 1999, 2011,  930,\n",
       "           711,  518, 1194,  385, 1902, 1726, 1992,   69, 1737,  459, 1657, 1740,\n",
       "           505, 1999,  692, 1870,  377, 1147,  762, 1518, 1515, 1112, 1104, 1299,\n",
       "           669,  687,  612,  783, 1178, 1149, 1559,  549,  693, 1902, 1152, 1354,\n",
       "           598, 1153,  672, 1457,  294,  267,  517,  251,  235,  799, 1349, 1959,\n",
       "          1590,  390,   26, 1975, 1288,  160, 1531,  323, 1730,  210,  701,  246,\n",
       "           126,  360, 1112, 1695,  305,  352, 1930, 2028,   16, 1439,  285, 1448,\n",
       "          1693,  898,   34, 1945,   47, 1916,  512, 2024,  762, 1745,  156,  372,\n",
       "          1682,  708, 1644,  586,  317,  836, 1363,  274, 1045,   95, 1728, 1856,\n",
       "           783],\n",
       "         [1493, 1162, 1443,   92, 1897,  946,  818, 1502,  131,   97, 1969,  284,\n",
       "          1028,   92,  153,  888,  303, 1717, 1147,  683, 1427, 1175, 1807,  536,\n",
       "          1981, 1982,  643,  827, 1581, 1918, 1042,  408,  323,  278, 1825,  610,\n",
       "           324,  591,   35,  481, 1183,  325, 1557,  536, 1372,   98,  275, 1330,\n",
       "          1353, 2040,   97,  744,  164,  164,  131, 1053,  933,  524, 1342,  223,\n",
       "          1057, 1858, 1261, 1171,  958,  234, 1780,  698,  364, 1147,  854,  116,\n",
       "           421,  447,  379, 2008,  536,  997,  325,   91, 1574,  791,  139, 2027,\n",
       "          1312,  791,  749,  870,  780, 1313, 1199, 1596, 1140, 1140,  524, 1936,\n",
       "           223,  346, 1498, 1646,  638, 1057, 1514, 1136, 1196, 1264,  254,   13,\n",
       "          1027,  366, 1514, 1057, 1858, 1673,  197, 1342,  947,  732, 1854,  131,\n",
       "           769],\n",
       "         [1134, 2001,  572, 1888, 1564,  324,  913, 1015, 1168,  340,  696,   31,\n",
       "           192, 2017,  540,  514,  835, 1213,  751,   31, 1584,  854, 1956, 1798,\n",
       "          1839,  788,  955, 1438, 2025,  749,   90, 1859,  691, 1584, 1411, 1311,\n",
       "          1741, 1135, 1258,   40, 1359,  670,  788, 1611,  711,  968,  577,  211,\n",
       "           209,   42,  823,  915,  581,  481, 1736,  578,   32,  610,  968,  968,\n",
       "          1230,  548,  142,  885, 1425,  717, 1005,  245, 1187, 1977,   56, 1146,\n",
       "           487,  259, 1251,  942,  816,  805,  285,  692, 1975,  462, 1680, 1925,\n",
       "          1607, 1738,  400,  899, 1944, 1757,  910, 1830, 1229, 1229, 1895, 1088,\n",
       "           347,   40, 1536, 1224,  975,  453,  258, 1704, 1904,  360,  242,  254,\n",
       "          1994, 1995,  971, 1928, 1450, 1982,  746,  503, 1114, 1499, 1890, 1015,\n",
       "          1616],\n",
       "         [ 931,   26, 1779, 1200,  386, 1351,  494, 1844, 1839, 1572,  224,  292,\n",
       "          1231,  881,   40, 1131,  118, 1000,  436, 1670, 1729,  937,  337, 1715,\n",
       "          1645,  419, 1167, 2018, 1154, 1566, 1214,   95,  333,  138,  171, 1567,\n",
       "          1831,  171, 1197, 1407,  933,  645,  759,  673, 1670,  739, 1403, 1142,\n",
       "          1474, 1497,  300,   54, 1443,  347, 1030, 1104, 1079,  965,  594,  155,\n",
       "          1488,  911,  307, 1798, 1859, 1582, 1080, 1676, 1734,  154,  303, 1522,\n",
       "          1222, 1318, 1027, 1704, 1979, 1186, 1274, 1147, 1699,    7, 1521, 1817,\n",
       "          1825,  932, 1986, 1245, 1873, 1407,  344, 1216, 2026, 1941, 1304, 1245,\n",
       "          1670,  514,  320, 1044, 1414, 1316, 1446,  899, 1314, 1729, 1245,  432,\n",
       "           526,  782,  686, 1556, 1306, 1021,  364,  677,  568,  966, 1797,  479,\n",
       "          1356],\n",
       "         [1968, 1018, 1671, 1327,  461,  433,  346,  225,  927,  513,  422, 1137,\n",
       "           863, 1018, 1990,  685, 1695, 1803, 2040,  422,  812, 1938, 1913,  115,\n",
       "          2028, 1217, 1106,  621,  650, 1359,  265, 1188,  231, 1369,  288, 1711,\n",
       "            74, 1164, 1500,  333,  784,  858, 1992, 1739,  801, 1795, 1441, 1813,\n",
       "          1779,  787,  839, 1509,  976, 1978,  825,   74, 1199, 1068,  256,  996,\n",
       "           349, 1647, 1577, 1735,  500,  366,  433, 1036,  890, 1728,  846, 1498,\n",
       "          1616,  697, 1556,  569,  609, 1530, 1306,  982, 1557, 1403,  622,  203,\n",
       "          1164,  703, 1983,  107, 1112,  130, 1327, 1412,   40,   86, 1397,  422,\n",
       "           107,  203, 1462,  893,  808, 1458,  361,  521,  866, 1926,  137,  388,\n",
       "           423,  492,  723,  429, 1660, 1273,  822, 1035,  410,  388,  924, 1966,\n",
       "           121],\n",
       "         [2017,  604,  314,   59, 1046,  191, 1719, 1715, 1190, 1551, 1917, 1876,\n",
       "          1740, 1264, 1880, 1974, 1310, 1059, 1841, 1350,  513,  244, 1513,  903,\n",
       "           212, 1326, 1727,  346,  745, 1261, 1327,  288,   80,  891,  237, 1757,\n",
       "           488,   24, 1399,  846, 1723,  890, 1233, 1202,  596,  767, 1724, 1998,\n",
       "          1757,  344, 1413,  507, 1180, 1180, 1744, 1294, 1706,  747, 1043, 1161,\n",
       "          1130,  422,  662, 1357,  784, 1887,   28, 1661,  508, 1298, 1904,  836,\n",
       "          2036,  407,  635,  502, 1981, 1343,  776,  546, 1785,  323,   58, 1527,\n",
       "          1652, 1820,  817,  201, 1871, 1649,  485,  523, 1858, 1535,  816, 1222,\n",
       "          1673,  312, 1650,  969, 1555,   79, 1622, 1806, 1142,  392,  372,   48,\n",
       "          1105,  547,   67, 2020, 1792,  670,  311, 1059, 1190,  329,  864, 1177,\n",
       "          1434]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the tokenizer by encoding and decoding some example text\n",
    "example_text = \"This is a test sentence.\"\n",
    "encoded = tokenizer(example_text, return_tensors=\"pt\")\n",
    "decoded = tokenizer.decode(encoded['input_ids'][0])\n",
    "\n",
    "# Print the results\n",
    "dataset[\"full\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def encode_text(role: str, content: str) -> torch.Tensor:\n",
    "    sys_line = tokenizer.encode(f\"<|im_start|>{role}\\n{content}<|im_end|>\\n\", return_tensors=\"pt\")\n",
    "    zeros_mask = torch.zeros(8, sys_line.size(1), dtype=sys_line.dtype)\n",
    "    return torch.cat([sys_line, zeros_mask])\n",
    "\n",
    "sysprompt = encode_text(\"system\", \"Speak out the provided text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>assistant\\n<|semantic:1698|><|semantic:1719|><|semantic:204|><|semantic:1389|><|semantic:851|><|semantic:1772|><|semantic:186|><|semantic:1307|><|semantic:1895|><|semantic:832|><|semantic:1633|><|semantic:771|><|semantic:648|><|semantic:1530|><|semantic:1989|><|semantic:1574|><|semantic:1348|><|semantic:722|><|semantic:144|><|semantic:1945|><|semantic:278|><|semantic:1109|><|semantic:29|><|semantic:611|><|semantic:46|><|semantic:622|><|semantic:628|><|semantic:1740|><|semantic:572|><|semantic:572|><|semantic:345|><|semantic:1989|><|semantic:1676|><|semantic:929|><|semantic:1776|><|semantic:749|><|semantic:313|><|semantic:1997|><|semantic:1571|><|semantic:819|><|semantic:1238|><|semantic:1054|><|semantic:1054|><|semantic:1135|><|semantic:1506|><|semantic:1393|><|semantic:616|><|semantic:1702|><|semantic:993|><|semantic:579|><|semantic:486|><|semantic:486|><|semantic:2039|><|semantic:148|><|semantic:657|><|semantic:664|><|semantic:339|><|semantic:339|><|semantic:588|><|semantic:212|><|semantic:1443|><|semantic:32|><|semantic:1320|><|semantic:1549|><|semantic:440|><|semantic:8|><|semantic:1407|><|semantic:1722|><|semantic:1650|><|semantic:1615|><|semantic:798|><|semantic:121|><|semantic:303|><|semantic:697|><|semantic:837|><|semantic:358|><|semantic:1882|><|semantic:440|><|semantic:1992|><|semantic:1992|><|semantic:587|><|semantic:178|><|semantic:178|><|semantic:1627|><|semantic:1530|><|semantic:929|><|semantic:1610|><|semantic:1916|><|semantic:523|><|semantic:213|><|semantic:1252|><|semantic:1480|><|semantic:1468|><|semantic:1899|><|semantic:773|><|semantic:2033|><|semantic:2033|><|semantic:83|><|semantic:1146|><|semantic:784|><|semantic:1295|><|semantic:199|><|semantic:1109|><|semantic:268|><|semantic:6|><|semantic:6|><|semantic:1781|><|semantic:1479|><|semantic:1530|><|semantic:1530|><|semantic:146|><|semantic:2038|><|semantic:984|><|semantic:1403|><|semantic:606|><|semantic:1379|><|semantic:1840|><|semantic:1172|><|semantic:1680|><|semantic:1162|><|semantic:1928|><|im_end|>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEMANTIC_OFFSET = tokenizer.encode(\"<|semantic:0|>\")[0]\n",
    "VQ_WRAPPER = encode_text(role=\"assistant\", content=\"\")[:,-7:-1]\n",
    "\n",
    "def encode_vq(codes: torch.Tensor) -> torch.Tensor:\n",
    "    speaker_line = codes[0,:] + SEMANTIC_OFFSET\n",
    "    vq_block = torch.cat([speaker_line.unsqueeze(0), codes])\n",
    "    return torch.cat([VQ_WRAPPER[:,:-1], vq_block, VQ_WRAPPER[:,-1].unsqueeze(1)], dim=1)\n",
    "\n",
    "\n",
    "out = encode_vq(dataset[\"full\"][0][\"codes\"])\n",
    "tokenizer.decode(out[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VQ_WRAPPER.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nSpeak out the provided text<|im_end|>\\n<|im_start|>user\\nPrinting, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1698|><|semantic:1719|><|semantic:204|><|semantic:1389|><|semantic:851|><|semantic:1772|><|semantic:186|><|semantic:1307|><|semantic:1895|><|semantic:832|><|semantic:1633|><|semantic:771|><|semantic:648|><|semantic:1530|><|semantic:1989|><|semantic:1574|><|semantic:1348|><|semantic:722|><|semantic:144|><|semantic:1945|><|semantic:278|><|semantic:1109|><|semantic:29|><|semantic:611|><|semantic:46|><|semantic:622|><|semantic:628|><|semantic:1740|><|semantic:572|><|semantic:572|><|semantic:345|><|semantic:1989|><|semantic:1676|><|semantic:929|><|semantic:1776|><|semantic:749|><|semantic:313|><|semantic:1997|><|semantic:1571|><|semantic:819|><|semantic:1238|><|semantic:1054|><|semantic:1054|><|semantic:1135|><|semantic:1506|><|semantic:1393|><|semantic:616|><|semantic:1702|><|semantic:993|><|semantic:579|><|semantic:486|><|semantic:486|><|semantic:2039|><|semantic:148|><|semantic:657|><|semantic:664|><|semantic:339|><|semantic:339|><|semantic:588|><|semantic:212|><|semantic:1443|><|semantic:32|><|semantic:1320|><|semantic:1549|><|semantic:440|><|semantic:8|><|semantic:1407|><|semantic:1722|><|semantic:1650|><|semantic:1615|><|semantic:798|><|semantic:121|><|semantic:303|><|semantic:697|><|semantic:837|><|semantic:358|><|semantic:1882|><|semantic:440|><|semantic:1992|><|semantic:1992|><|semantic:587|><|semantic:178|><|semantic:178|><|semantic:1627|><|semantic:1530|><|semantic:929|><|semantic:1610|><|semantic:1916|><|semantic:523|><|semantic:213|><|semantic:1252|><|semantic:1480|><|semantic:1468|><|semantic:1899|><|semantic:773|><|semantic:2033|><|semantic:2033|><|semantic:83|><|semantic:1146|><|semantic:784|><|semantic:1295|><|semantic:199|><|semantic:1109|><|semantic:268|><|semantic:6|><|semantic:6|><|semantic:1781|><|semantic:1479|><|semantic:1530|><|semantic:1530|><|semantic:146|><|semantic:2038|><|semantic:984|><|semantic:1403|><|semantic:606|><|semantic:1379|><|semantic:1840|><|semantic:1172|><|semantic:1680|><|semantic:1162|><|semantic:1928|>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def tokenize_row(row: Dict) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    row[\"normalized_text\"] is a string\n",
    "    row[\"codes\"] is a torch.Tensor shaped [9, T_vq]\n",
    "    \"\"\"\n",
    "    user_line = encode_text(role=\"user\", content=row[\"normalized_text\"])\n",
    "    codes_9rows = encode_vq(row[\"codes\"])  # shape [9, T_vq]\n",
    "    \n",
    "    # Concatenate system prompt (row=1?), user line (row=1?), codebooks (row=9),\n",
    "    # but along the *time* dimension => final shape [9, T_total] \n",
    "    #   (since sysprompt and user_line are [1, T_something], \n",
    "    #    codes_9rows is [9, T_vq], so we pad them to 9 rows if needed)\n",
    "    # For demonstration, I'm just stacking them. You probably do:\n",
    "    tokens = torch.cat([sysprompt, user_line, codes_9rows], dim=1)\n",
    "    # tokens shape => [9, big_T]\n",
    "    \n",
    "    # Clone for labels\n",
    "    labels = tokens.clone()\n",
    "\n",
    "    # Let's define the \"text portion\" as sysprompt + user_line only\n",
    "    text_len = sysprompt.size(1) + user_line.size(1)  # no VQ_WRAPPER or codes\n",
    "\n",
    "    # ONLY mask codebook rows for that text region\n",
    "    # row=0 is your \"text\" row, row=1..8 might be codebooks, or vice versa\n",
    "    # (Here I'm assuming row=0 is your actual text tokens. \n",
    "    #  If it's reversed, tweak accordingly!)\n",
    "    labels[1:, :text_len] = -100\n",
    "\n",
    "    # Also mask the final <|im_end|> if that’s how you do it:\n",
    "    labels[1:, -1] = -100\n",
    "\n",
    "    # Usually you drop the final time-step from tokens\n",
    "    return {\n",
    "        \"tokens\": tokens[:, :-1],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "tokenizer.decode(tokenize_row(dataset[\"full\"][0])[\"tokens\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13100/13100 [00:07<00:00, 1856.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(tokenize_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'lj-001-0001.wav',\n",
       " 'spoken_text': 'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition',\n",
       " 'normalized_text': 'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition',\n",
       " 'codes': tensor([[1698, 1719,  204, 1389,  851, 1772,  186, 1307, 1895,  832, 1633,  771,\n",
       "           648, 1530, 1989, 1574, 1348,  722,  144, 1945,  278, 1109,   29,  611,\n",
       "            46,  622,  628, 1740,  572,  572,  345, 1989, 1676,  929, 1776,  749,\n",
       "           313, 1997, 1571,  819, 1238, 1054, 1054, 1135, 1506, 1393,  616, 1702,\n",
       "           993,  579,  486,  486, 2039,  148,  657,  664,  339,  339,  588,  212,\n",
       "          1443,   32, 1320, 1549,  440,    8, 1407, 1722, 1650, 1615,  798,  121,\n",
       "           303,  697,  837,  358, 1882,  440, 1992, 1992,  587,  178,  178, 1627,\n",
       "          1530,  929, 1610, 1916,  523,  213, 1252, 1480, 1468, 1899,  773, 2033,\n",
       "          2033,   83, 1146,  784, 1295,  199, 1109,  268,    6,    6, 1781, 1479,\n",
       "          1530, 1530,  146, 2038,  984, 1403,  606, 1379, 1840, 1172, 1680, 1162,\n",
       "          1928],\n",
       "         [1293, 1310, 1535, 1766, 1004, 1313,  448, 1253,  907, 1050, 1153, 1122,\n",
       "          1077, 1279,  523, 1624, 1839,  122, 1537, 1101,  666,  159, 1523, 1101,\n",
       "           196,  298, 1234, 1958, 1668, 1094,  551, 1157, 1461, 2047,  410,  254,\n",
       "           783, 1410,   28,  714,  921,  421, 1462, 1995, 1125, 1847, 2032, 1011,\n",
       "          1523, 1546, 1211, 1050, 1056, 1056, 1056, 1611,  250, 1264, 1742,   57,\n",
       "          2024, 1496, 1801, 1638, 1985, 1839, 1681, 1138,  561, 2024,  302, 1027,\n",
       "          1776, 1539, 1138,  549, 1197, 1019, 1348,  254,  931,  892, 1019, 1836,\n",
       "          1004,  351, 1246, 1862, 1800,  884, 1349,  138,  643,  982, 1188,  497,\n",
       "          1143,  724, 1681,  815, 1228, 1876, 1911, 1668, 1615,  786, 1519,  690,\n",
       "          1750, 1212, 1270,  352,    9, 1276, 2035, 1231, 1530, 1232,  448,  907,\n",
       "          1159],\n",
       "         [ 850,  901, 1258,  692,  532,  632,  988,  462,  462, 1697,  259, 1992,\n",
       "           875,  229, 1769, 1087, 1725, 1353, 1738,  692,  407, 1999, 2011,  930,\n",
       "           711,  518, 1194,  385, 1902, 1726, 1992,   69, 1737,  459, 1657, 1740,\n",
       "           505, 1999,  692, 1870,  377, 1147,  762, 1518, 1515, 1112, 1104, 1299,\n",
       "           669,  687,  612,  783, 1178, 1149, 1559,  549,  693, 1902, 1152, 1354,\n",
       "           598, 1153,  672, 1457,  294,  267,  517,  251,  235,  799, 1349, 1959,\n",
       "          1590,  390,   26, 1975, 1288,  160, 1531,  323, 1730,  210,  701,  246,\n",
       "           126,  360, 1112, 1695,  305,  352, 1930, 2028,   16, 1439,  285, 1448,\n",
       "          1693,  898,   34, 1945,   47, 1916,  512, 2024,  762, 1745,  156,  372,\n",
       "          1682,  708, 1644,  586,  317,  836, 1363,  274, 1045,   95, 1728, 1856,\n",
       "           783],\n",
       "         [1493, 1162, 1443,   92, 1897,  946,  818, 1502,  131,   97, 1969,  284,\n",
       "          1028,   92,  153,  888,  303, 1717, 1147,  683, 1427, 1175, 1807,  536,\n",
       "          1981, 1982,  643,  827, 1581, 1918, 1042,  408,  323,  278, 1825,  610,\n",
       "           324,  591,   35,  481, 1183,  325, 1557,  536, 1372,   98,  275, 1330,\n",
       "          1353, 2040,   97,  744,  164,  164,  131, 1053,  933,  524, 1342,  223,\n",
       "          1057, 1858, 1261, 1171,  958,  234, 1780,  698,  364, 1147,  854,  116,\n",
       "           421,  447,  379, 2008,  536,  997,  325,   91, 1574,  791,  139, 2027,\n",
       "          1312,  791,  749,  870,  780, 1313, 1199, 1596, 1140, 1140,  524, 1936,\n",
       "           223,  346, 1498, 1646,  638, 1057, 1514, 1136, 1196, 1264,  254,   13,\n",
       "          1027,  366, 1514, 1057, 1858, 1673,  197, 1342,  947,  732, 1854,  131,\n",
       "           769],\n",
       "         [1134, 2001,  572, 1888, 1564,  324,  913, 1015, 1168,  340,  696,   31,\n",
       "           192, 2017,  540,  514,  835, 1213,  751,   31, 1584,  854, 1956, 1798,\n",
       "          1839,  788,  955, 1438, 2025,  749,   90, 1859,  691, 1584, 1411, 1311,\n",
       "          1741, 1135, 1258,   40, 1359,  670,  788, 1611,  711,  968,  577,  211,\n",
       "           209,   42,  823,  915,  581,  481, 1736,  578,   32,  610,  968,  968,\n",
       "          1230,  548,  142,  885, 1425,  717, 1005,  245, 1187, 1977,   56, 1146,\n",
       "           487,  259, 1251,  942,  816,  805,  285,  692, 1975,  462, 1680, 1925,\n",
       "          1607, 1738,  400,  899, 1944, 1757,  910, 1830, 1229, 1229, 1895, 1088,\n",
       "           347,   40, 1536, 1224,  975,  453,  258, 1704, 1904,  360,  242,  254,\n",
       "          1994, 1995,  971, 1928, 1450, 1982,  746,  503, 1114, 1499, 1890, 1015,\n",
       "          1616],\n",
       "         [ 931,   26, 1779, 1200,  386, 1351,  494, 1844, 1839, 1572,  224,  292,\n",
       "          1231,  881,   40, 1131,  118, 1000,  436, 1670, 1729,  937,  337, 1715,\n",
       "          1645,  419, 1167, 2018, 1154, 1566, 1214,   95,  333,  138,  171, 1567,\n",
       "          1831,  171, 1197, 1407,  933,  645,  759,  673, 1670,  739, 1403, 1142,\n",
       "          1474, 1497,  300,   54, 1443,  347, 1030, 1104, 1079,  965,  594,  155,\n",
       "          1488,  911,  307, 1798, 1859, 1582, 1080, 1676, 1734,  154,  303, 1522,\n",
       "          1222, 1318, 1027, 1704, 1979, 1186, 1274, 1147, 1699,    7, 1521, 1817,\n",
       "          1825,  932, 1986, 1245, 1873, 1407,  344, 1216, 2026, 1941, 1304, 1245,\n",
       "          1670,  514,  320, 1044, 1414, 1316, 1446,  899, 1314, 1729, 1245,  432,\n",
       "           526,  782,  686, 1556, 1306, 1021,  364,  677,  568,  966, 1797,  479,\n",
       "          1356],\n",
       "         [1968, 1018, 1671, 1327,  461,  433,  346,  225,  927,  513,  422, 1137,\n",
       "           863, 1018, 1990,  685, 1695, 1803, 2040,  422,  812, 1938, 1913,  115,\n",
       "          2028, 1217, 1106,  621,  650, 1359,  265, 1188,  231, 1369,  288, 1711,\n",
       "            74, 1164, 1500,  333,  784,  858, 1992, 1739,  801, 1795, 1441, 1813,\n",
       "          1779,  787,  839, 1509,  976, 1978,  825,   74, 1199, 1068,  256,  996,\n",
       "           349, 1647, 1577, 1735,  500,  366,  433, 1036,  890, 1728,  846, 1498,\n",
       "          1616,  697, 1556,  569,  609, 1530, 1306,  982, 1557, 1403,  622,  203,\n",
       "          1164,  703, 1983,  107, 1112,  130, 1327, 1412,   40,   86, 1397,  422,\n",
       "           107,  203, 1462,  893,  808, 1458,  361,  521,  866, 1926,  137,  388,\n",
       "           423,  492,  723,  429, 1660, 1273,  822, 1035,  410,  388,  924, 1966,\n",
       "           121],\n",
       "         [2017,  604,  314,   59, 1046,  191, 1719, 1715, 1190, 1551, 1917, 1876,\n",
       "          1740, 1264, 1880, 1974, 1310, 1059, 1841, 1350,  513,  244, 1513,  903,\n",
       "           212, 1326, 1727,  346,  745, 1261, 1327,  288,   80,  891,  237, 1757,\n",
       "           488,   24, 1399,  846, 1723,  890, 1233, 1202,  596,  767, 1724, 1998,\n",
       "          1757,  344, 1413,  507, 1180, 1180, 1744, 1294, 1706,  747, 1043, 1161,\n",
       "          1130,  422,  662, 1357,  784, 1887,   28, 1661,  508, 1298, 1904,  836,\n",
       "          2036,  407,  635,  502, 1981, 1343,  776,  546, 1785,  323,   58, 1527,\n",
       "          1652, 1820,  817,  201, 1871, 1649,  485,  523, 1858, 1535,  816, 1222,\n",
       "          1673,  312, 1650,  969, 1555,   79, 1622, 1806, 1142,  392,  372,   48,\n",
       "          1105,  547,   67, 2020, 1792,  670,  311, 1059, 1190,  329,  864, 1177,\n",
       "          1434]]),\n",
       " 'tokens': tensor([[    1,  9690,   198,  ..., 50832, 50314, 51080],\n",
       "         [    0,     0,     0,  ...,  1680,  1162,  1928],\n",
       "         [    0,     0,     0,  ...,   448,   907,  1159],\n",
       "         ...,\n",
       "         [    0,     0,     0,  ...,  1797,   479,  1356],\n",
       "         [    0,     0,     0,  ...,   924,  1966,   121],\n",
       "         [    0,     0,     0,  ...,   864,  1177,  1434]]),\n",
       " 'labels': tensor([[    1,  9690,   198,  ..., 50314, 51080,     2],\n",
       "         [ -100,  -100,  -100,  ...,  1162,  1928,  -100],\n",
       "         [ -100,  -100,  -100,  ...,   907,  1159,  -100],\n",
       "         ...,\n",
       "         [ -100,  -100,  -100,  ...,   479,  1356,  -100],\n",
       "         [ -100,  -100,  -100,  ...,  1966,   121,  -100],\n",
       "         [ -100,  -100,  -100,  ...,  1177,  1434,  -100]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"full\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 13100/13100 [00:00<00:00, 17090.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"tokenized_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
